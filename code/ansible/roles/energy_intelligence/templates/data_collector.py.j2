#!/usr/bin/env python3
"""
Data Collector - Collecteur de donn√©es √©nerg√©tiques
File: {{ energy_ai_paths.base | default('/opt/energy-ai') }}/data_collector.py
Generated by Ansible - Energy Intelligence Role
"""

import json
import time
import logging
import pandas as pd
from datetime import datetime, timedelta
from influxdb_client import InfluxDBClient
import sys
import os

# Configuration
INFLUXDB_URL = "{{ influxdb_url | default('http://localhost:8086') }}"
INFLUXDB_TOKEN = "{{ influxdb_token.content | b64decode | trim if influxdb_token is defined else 'demo-token' }}"
INFLUXDB_ORG = "{{ influxdb_org | default('iot') }}"

# Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('DataCollector')

def collect_historical_data(bucket, measurement, time_range_hours=24):
    """Collection des donn√©es historiques depuis InfluxDB"""
    try:
        client = InfluxDBClient(url=INFLUXDB_URL, token=INFLUXDB_TOKEN, org=INFLUXDB_ORG)
        query_api = client.query_api()
        
        query = f'''
        from(bucket: "{bucket}")
        |> range(start: -{time_range_hours}h)
        |> filter(fn: (r) => r._measurement == "{measurement}")
        |> aggregateWindow(every: 1h, fn: mean, createEmpty: false)
        |> pivot(rowKey:["_time"], columnKey: ["_field"], valueColumn: "_value")
        '''
        
        logger.info(f"üìä Collecte {measurement} depuis {bucket} ({time_range_hours}h)")
        
        result = query_api.query(query)
        data = []
        
        for table in result:
            for record in table.records:
                row = {'timestamp': record.get_time()}
                for field in record.values:
                    if field.startswith('_') and field not in ['_time', '_start', '_stop']:
                        continue
                    if field not in ['result', 'table', '_measurement']:
                        row[field] = record.values[field]
                data.append(row)
        
        df = pd.DataFrame(data)
        logger.info(f"‚úÖ {len(df)} enregistrements collect√©s")
        
        client.close()
        return df
        
    except Exception as e:
        logger.error(f"‚ùå Erreur collecte {measurement}: {e}")
        return pd.DataFrame()

def collect_energy_data():
    """Collection des donn√©es √©nerg√©tiques"""
    logger.info("üîã Collection des donn√©es √©nerg√©tiques")
    
    datasets = {}
    
    # Donn√©es de consommation
    consumption_df = collect_historical_data("telegraf", "energy_consumption", 168)  # 7 jours
    if not consumption_df.empty:
        datasets['consumption'] = consumption_df
    
    # Donn√©es de production solaire
    solar_df = collect_historical_data("telegraf", "solar_production", 168)
    if not solar_df.empty:
        datasets['solar'] = solar_df
    
    # Donn√©es de batterie
    battery_df = collect_historical_data("telegraf", "battery_status", 168)
    if not battery_df.empty:
        datasets['battery'] = battery_df
    
    return datasets

def collect_weather_data():
    """Collection des donn√©es m√©t√©orologiques"""
    logger.info("üå§Ô∏è Collection des donn√©es m√©t√©o")
    
    weather_df = collect_historical_data("weather_ml", "weather_data", 168)
    return weather_df

def collect_ai_performance_data():
    """Collection des donn√©es de performance IA"""
    logger.info("üß† Collection des donn√©es de performance IA")
    
    datasets = {}
    
    # Pr√©dictions IA
    predictions_df = collect_historical_data("energy_ai", "energy_ai_predictions", 72)  # 3 jours
    if not predictions_df.empty:
        datasets['predictions'] = predictions_df
    
    # Sc√©narios ex√©cut√©s
    scenarios_df = collect_historical_data("energy_scenarios", "energy_scenarios_executed", 72)
    if not scenarios_df.empty:
        datasets['scenarios'] = scenarios_df
    
    # Optimisation des co√ªts
    costs_df = collect_historical_data("cost_optimization", "cost_optimization", 72)
    if not costs_df.empty:
        datasets['costs'] = costs_df
    
    return datasets

def export_datasets(datasets, format='csv'):
    """Export des datasets collect√©s"""
    export_dir = "{{ energy_ai_paths.data | default('/opt/energy-ai/data') }}/exports"
    os.makedirs(export_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    exported_files = []
    
    for name, df in datasets.items():
        if df.empty:
            continue
            
        filename = f"{name}_{timestamp}.{format}"
        filepath = os.path.join(export_dir, filename)
        
        try:
            if format == 'csv':
                df.to_csv(filepath, index=False)
            elif format == 'json':
                df.to_json(filepath, orient='records', date_format='iso')
            elif format == 'parquet':
                df.to_parquet(filepath, index=False)
            
            exported_files.append(filepath)
            logger.info(f"üìÅ Export {name}: {filepath} ({len(df)} lignes)")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur export {name}: {e}")
    
    return exported_files

def generate_data_summary(datasets):
    """G√©n√©ration d'un r√©sum√© des donn√©es collect√©es"""
    summary = {
        'collection_timestamp': datetime.now().isoformat(),
        'datasets': {},
        'total_records': 0,
        'time_range': {
            'start': None,
            'end': None
        }
    }
    
    all_timestamps = []
    
    for name, df in datasets.items():
        if df.empty:
            continue
            
        dataset_info = {
            'records_count': len(df),
            'columns': list(df.columns),
            'time_range': {
                'start': df['timestamp'].min().isoformat() if 'timestamp' in df.columns else None,
                'end': df['timestamp'].max().isoformat() if 'timestamp' in df.columns else None
            },
            'data_types': {col: str(dtype) for col, dtype in df.dtypes.items()}
        }
        
        summary['datasets'][name] = dataset_info
        summary['total_records'] += len(df)
        
        if 'timestamp' in df.columns:
            all_timestamps.extend(df['timestamp'].tolist())
    
    if all_timestamps:
        summary['time_range']['start'] = min(all_timestamps).isoformat()
        summary['time_range']['end'] = max(all_timestamps).isoformat()
    
    return summary

def main():
    """Point d'entr√©e principal"""
    print("üìä DATA COLLECTOR - ENERGY AI")
    print("=" * 50)
    print(f"üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        all_datasets = {}
        
        # Collection des diff√©rents types de donn√©es
        print("\nüîã COLLECTE DONN√âES √âNERG√âTIQUES")
        print("-" * 30)
        energy_data = collect_energy_data()
        all_datasets.update(energy_data)
        
        print("\nüå§Ô∏è COLLECTE DONN√âES M√âT√âO")
        print("-" * 30)
        weather_data = collect_weather_data()
        if not weather_data.empty:
            all_datasets['weather'] = weather_data
        
        print("\nüß† COLLECTE DONN√âES PERFORMANCE IA")
        print("-" * 30)
        ai_data = collect_ai_performance_data()
        all_datasets.update(ai_data)
        
        # G√©n√©ration du r√©sum√©
        summary = generate_data_summary(all_datasets)
        
        # Export des donn√©es
        print("\nüìÅ EXPORT DES DONN√âES")
        print("-" * 30)
        exported_files = export_datasets(all_datasets, 'csv')
        
        # Sauvegarde du r√©sum√©
        summary_path = "{{ energy_ai_paths.data | default('/opt/energy-ai/data') }}/collection_summary.json"
        with open(summary_path, 'w') as f:
            json.dump(summary, f, indent=2)
        
        print("\n‚úÖ COLLECTE TERMIN√âE")
        print("=" * 50)
        print(f"üìä Datasets collect√©s: {len(all_datasets)}")
        print(f"üìã Total enregistrements: {summary['total_records']}")
        print(f"üìÅ Fichiers export√©s: {len(exported_files)}")
        print(f"üìÑ R√©sum√©: {summary_path}")
        
        if summary['time_range']['start']:
            print(f"‚è∞ P√©riode: {summary['time_range']['start']} ‚Üí {summary['time_range']['end']}")
        
        print("\nüí° Fichiers export√©s:")
        for file in exported_files:
            print(f"   ‚Ä¢ {file}")
        
        return 0
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la collecte: {e}")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)